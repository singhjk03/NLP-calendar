{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohit\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"model\")\n",
    "\n",
    "\n",
    "##### Process text sample (from wikipedia)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'DATE',\n",
       "  'score': 0.917465,\n",
       "  'word': '24th jan',\n",
       "  'start': 32,\n",
       "  'end': 41},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9353217,\n",
       "  'word': 'Lake view hotel',\n",
       "  'start': 48,\n",
       "  'end': 64}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"Schedule a meeting with mohit on 24th jan at the Lake view hotel at 2pm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"djagatiya/ner-bert-base-cased-ontonotesv5-englishv4\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"djagatiya/ner-bert-base-cased-ontonotesv5-englishv4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_date\\\\tokenizer_config.json',\n",
       " 'tokenizer_date\\\\special_tokens_map.json',\n",
       " 'tokenizer_date\\\\vocab.txt',\n",
       " 'tokenizer_date\\\\added_tokens.json',\n",
       " 'tokenizer_date\\\\tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"model_date\")\n",
    "tokenizer.save_pretrained(\"tokenizer_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     40\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchedule a meeting with mohit on 24th Jan at 2pm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 41\u001b[0m entities \u001b[38;5;241m=\u001b[39m \u001b[43midentify_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Print the identified entities\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(entities)\n",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m, in \u001b[0;36midentify_entities\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     15\u001b[0m entity_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token, prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(decoded_tokens, predictions\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mtolist()):\n\u001b[0;32m     17\u001b[0m   \u001b[38;5;66;03m# Check for B- or I- tags to identify entity start and continuation\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mprediction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     entity_type \u001b[38;5;241m=\u001b[39m prediction[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Extract entity type from label (e.g., B-LOC)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     entity_name \u001b[38;5;241m=\u001b[39m token  \u001b[38;5;66;03m# Start a new entity\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "\n",
    "# Define a function to process the sentence and return entities\n",
    "def identify_entities(sentence):\n",
    "  encoded_input = tokenizer(sentence, return_tensors=\"pt\")  # Tokenize and convert to tensors\n",
    "  with torch.no_grad():  # Disable gradient calculation for faster inference\n",
    "    outputs = model(**encoded_input)  # Pass input through the model\n",
    "    logits = outputs.logits  # Get the logits\n",
    "    predictions = torch.argmax(logits, dim=-1)  # Get the most likely label for each token (already integers)\n",
    "\n",
    "  decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"].squeeze().tolist())\n",
    "\n",
    "  # Create a list to store entities\n",
    "  entities = []\n",
    "  entity_name = \"\"\n",
    "  entity_type = \"\"\n",
    "  for token, prediction in zip(decoded_tokens, predictions.squeeze().tolist()):\n",
    "    # Check for B- or I- tags to identify entity start and continuation\n",
    "    if prediction.startswith(\"B-\"):\n",
    "      entity_type = prediction[2:]  # Extract entity type from label (e.g., B-LOC)\n",
    "      entity_name = token  # Start a new entity\n",
    "    elif prediction.startswith(\"I-\"):\n",
    "      # If the type matches the current entity, continue adding tokens\n",
    "      if entity_type == prediction[2:]:\n",
    "        entity_name += \" \" + token\n",
    "      else:\n",
    "        # Different entity type encountered, save the previous entity and start a new one\n",
    "        entities.append((entity_name, entity_type))\n",
    "        entity_type = prediction[2:]\n",
    "        entity_name = token\n",
    "    # Check for single-word entities (O tag not required)\n",
    "    elif token != \"[PAD]\":  # Ignore padding tokens\n",
    "      entities.append((token, \"O\"))  # \"O\" indicates \"Outside\" of an entity\n",
    "  # Add the last entity if it wasn't saved yet\n",
    "  if entity_name:\n",
    "    entities.append((entity_name, entity_type))\n",
    "\n",
    "  return entities\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Schedule a meeting with mohit on 24th Jan at 2pm\"\n",
    "entities = identify_entities(sentence)\n",
    "\n",
    "# Print the identified entities\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"token-classification\", model=\"djagatiya/ner-bert-base-cased-ontonotesv5-englishv4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '28th Jan', 'time': '2'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_date_time(entities):\n",
    "  \"\"\"\n",
    "  Extracts date and time entities from the model output.\n",
    "\n",
    "  Args:\n",
    "      entities: A list of dictionaries representing model predictions.\n",
    "          Each dictionary contains information like 'entity' type, 'word', etc.\n",
    "\n",
    "  Returns:\n",
    "      A dictionary with keys 'date' and 'time' containing the identified date and time,\n",
    "      or None if none are found.\n",
    "  \"\"\"\n",
    "  date_entity = None\n",
    "  time_entity = None\n",
    "  for entity in entities:\n",
    "    # Consider only B-DATE or B-TIME entities\n",
    "    if entity['entity'].startswith('B-'):\n",
    "      # If B-DATE encountered, reset time_entity and store date\n",
    "      if entity['entity'] == 'B-DATE':\n",
    "        time_entity = None\n",
    "        date_entity = entity['word']\n",
    "      # If B-TIME encountered, store time only if no previous I-TIME was seen\n",
    "      elif entity['entity'] == 'B-TIME' and not time_entity:\n",
    "        time_entity = entity['word']\n",
    "\n",
    "  # Combine date parts if necessary (assuming '28th' and 'Jan' are separate entities)\n",
    "#   if date_entity and entities[entities.index(entity) + 1]['entity'] == 'I-DATE':\n",
    "#     date_entity += \" \" + entities[entities.index(entity) + 1]['word']\n",
    "    if date_entity and entities.index(entity) < len(entities) - 1:  # Check if next element exists\n",
    "        next_entity = entities[entities.index(entity) + 1]\n",
    "        if next_entity['entity'] == 'I-DATE':\n",
    "            date_entity += \" \" + next_entity['word']\n",
    "\n",
    "  return {'date': date_entity, 'time': time_entity}\n",
    "\n",
    "# Example usage:\n",
    "# model_output = [{'entity': 'B-DATE', ...}, ...] \n",
    "sentence = pipe(\"I have a meeting with the CEO on 28th Jan at 2AM\")\n",
    "date_time_info = extract_date_time(sentence)\n",
    "print(date_time_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
